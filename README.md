# qdocs-search - Quantinuum Docs Search MVP

A full-stack, server-side search engine built for Quantinuum's technical documentation.

## ðŸš€ Setup & Run Instructions

**Prerequisites:** Node.js (v18+)

### 1. **Install dependencies:**
   \`\`\`bash
   npm install
   \`\`\`

### 2. How to Add Documentation Projects
The search engine is designed to dynamically discover and index Sphinx projects. To add a new documentation site, create a new folder under the `public/` directory (e.g., `public/my-project/`) and populate it:

- **`searchindex.js` (Required):** The raw search index generated by Sphinx.
- **`projectInfo.json` (Optional):** UI metadata to make the frontend look nice. Example:
  \`\`\`json
  {
    "externalBaseUrl": "https://qiskit-community.github.io/qiskit-nature/",
    "suggestedLinks": [
      {
        "title": "Getting Started",
        "path": "/getting_started.html",
        "subtitle": "Install Qiskit Finance and run your first simulation."
      }
    ]
  }
  \`\`\`
- **Full Sphinx HTML Build (Optional):** If you include the rest of the generated HTML files (not only `searchindex.js`), the build pipeline will automatically copy them to the Next.js `public/` folder so they are served locally. If omitted, the UI gracefully falls back to linking the user to the `externalBaseUrl` specified in `projectInfo.json`. If that is not present, a 404 error is shown.

*Example directory structure:*
\`\`\`text
qdocs-search/
â””â”€â”€ public/
    â””â”€â”€ qiskit-nature/
        â”œâ”€â”€ searchindex.js         (Required)
        â”œâ”€â”€ projectInfo.json       (Optional UI metadata)
        â”œâ”€â”€ index.html             (Optional local docs)
        â””â”€â”€ _static/               (Optional local docs)
        ...                        (Other Optional local docs)
\`\`\`

### 3. Build the Combined Search Index
Once your data folders are set up, run the pipeline script. This parses all `searchindex.js` files, builds the highly optimized inverted index (`data/combined-searchindex.json`), and prepares it for the API.
\`\`\`bash
npm run build:search-index
\`\`\`

### 4. **Run the Development Server:**
   \`\`\`bash
   npm run dev
   \`\`\`

### 5. **Open the App:** Navigate to [http://localhost:3000](http://localhost:3000)

---

## ðŸ—ï¸ Architecture & Schema Experimentation

A core requirement was to experiment with different JSON schema designs for the search index.

### Schema Experimentation
Initially, I considered a **Flat Document Array (Schema A)**: `[{ title, terms: [...], project }]`. While simple, filtering and searching requires an $O(N)$ scan of every term in every document on every keystroke, leading to poor scaling characteristics.

I opted for an **Inverted Index (Schema B)**. The pipeline script (`scripts/combine-search-indexes.ts`) pre-computes an inverted index mapping stemmed tokens directly to document IDs. 

**Why Schema B won:**
- **Payload Size:** Significantly smaller on disk and in memory.
- **Lookup Speed:** Searching for a term is $O(1)$ dictionary lookup rather than an $O(N)$ scan.
- **Namespacing:** Document IDs are prefixed with their project slug (e.g., `qiskit-nature:42`), avoiding collisions across subsites and enabling $O(1)$ project filtering.

---

## ðŸ§  Ranking Heuristics

The search engine (`src/lib/search-engine.ts`) implements a custom scoring algorithm prioritizing exact matches, headings, and deep links:

1. **Tokenization & Stemming:** Queries are stripped of stop-words and stemmed using the Porter Stemmer (`natural` npm package) to match Sphinx's pre-stemmed term keys.
2. **Term Weights:** 
   - Body matches (`terms`) = `1.0`
   - Heading matches (`titleterms`) = `3.0`
3. **Smooth IDF (Inverse Document Frequency):** Terms that appear across fewer documents are weighted higher: `weight * (log(totalDocs / docsWithTerm + 1) + 1)`.
4. **Fuzzy Fallback (Typo Tolerance):** If an exact stem isn't found, the engine falls back to Jaro-Winkler distance. If similarity is `>= 0.88`, the match is accepted but the score is scaled down by the similarity penalty.
5. **Section Deep-Linking Bonus:** If query tokens match section headings (`alltitles`), a +2.0 bonus is applied, and the UI exposes a deep-link anchor directly to that section.
6. **All-Tokens Multiplier:** Documents containing *all* query tokens receive a 1.25x final score multiplier.

---

## ðŸ“Š Analytics & Telemetry

A simple, in-memory telemetry module (`src/lib/telemetry.ts`) tracks:
- `search_performed`: Query, result count, project filter.
- `result_selected`: Document clicked and its rank position.

**Data Thinking:** To prevent memory leaks in a long-running Node process, telemetry events are stored in heavily-typed **Circular Buffers** bounded to 1,000 events. 
*You can view the raw telemetry output at:* `GET /api/telemetry`

---

## UX & Accessibility (a11y)

- **ARIA Combobox Pattern:** The search input implements standard `role="combobox"`, `aria-expanded`, and `aria-activedescendant` attributes.
- **Keyboard Navigation:** Full support for `ArrowUp`, `ArrowDown`, `Enter`, and `Escape`. Highlighted items are automatically scrolled into view (`scrollIntoView`).
- **Debouncing:** Queries are debounced by 300ms to preserve API performance.
- **Progressive Fallbacks:** If a Sphinx HTML build is missing, the UI gracefully falls back to linking to external documentation URLs (`projectInfo.json` config).

---

## âš–ï¸ Assumptions & Trade-offs

1. **UI Library:** The prompt suggested "redux-ui components". Assuming this was a typo for *Radix UI* or a deprecated internal library, I elected to use **Material UI (MUI)**. This allowed me to rapidly build a highly accessible, enterprise-grade combobox so I could focus my time on the search algorithms and inverted index data structures.
2. **Snippets:** Sphinx's `searchindex.js` only provides arrays of stemmed terms, not the full source text. True text snippets weren't possible without parsing the raw HTML at runtime. Instead, the UI highlights the matched terms and surfaces matched section headings to provide context.
3. **In-Memory Telemetry:** Telemetry is lost on server restart. In production, this would be swapped for a Redis/Kafka stream.
4. **MDX vs Sphinx HTML:** The prompt mentioned indexing "a small set of MDX pages", but also explicitly required using the `searchindex.js` generated by Sphinx (which compiles `.rst` to `.html`). Because I used Sphinx indexes for Qiskit Nature and Machine Learning, the pipeline currently serves and deep-links directly to the static Sphinx HTML builds rather than rendering MDX. The search architecture, however, is entirely agnostic to the rendering layerâ€”if MDX files were provided alongside their search indexes, the exact same `CombinedSearchIndex` schema and UI would work seamlessly.

---

## Next Steps

1. **Vector Search (Embeddings):** While TF-IDF is great for exact/fuzzy keyword matches, semantic search (e.g., matching "machine learning" to "AI") requires vector embeddings. I would pipe the Sphinx output through OpenAI/Cohere to generate embeddings and store them in Pinecone or pgvector.
2. **GitHub Actions / CI/CD:** Automate the `combine-search-indexes` script to run in a GitHub action triggered by webhooks from the upstream documentation repositories.
3. **Synonym Dictionary:** Implement a custom config file mapping common acronyms (e.g., `QEC` -> `error correction`) and inject them during the tokenization phase.
